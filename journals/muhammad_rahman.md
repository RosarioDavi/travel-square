11/22
Today, I tried to implement a keyword search for people's usernames. Basically, I want other users to be able to send a string and if any username matches the letters of that string, they should pop up. Tried multiple ways, but it just doesn't work with a placeholder. Might have to scrap it in place of a get_all_users function. However, that is very lame, so I'll try to consult Riley on it. I also did some pair programming with Rosario to help him with get_all_venues and create_venues. I implemented the join table functionality for the get query, and it can easily be understood and translated for other get requests for venues and even reviews. Good learning experience for the whole team. This is actually fun to do.

11/21
Forgot to do my journal. This day was very tiring. Figuring out how Auth would work for the parameters our accounts take in was difficult. The way that it is currently set up, hashed passwords also leak out in the GET requests for accounts. This could be a major security issue in the real world. However, I will consider this a success nonetheless that will be fixed in a future update after the MVP is done. Perhaps, I will create an Account class on top of AccountIn and AccountOut that will be used to retrieve account information. I say that this is a security issue because users should be able to find other users and even though they won't see the hashed passwords in the frontend screen, a crafty hacker may figure out the exploit. Currently leaving this fix for later.

11/18
WE MADE SO MUCH PROGRESS TODAY!!! I was able to quickly recover all the work we did and translate it into Postgres. I made all the tables and got started on the accounts routers and queries. We're dividing up accounts/venues/reviews/requests each for one person. I'm really starting to get the hang of writing FastAPI code. I really do miss using Django, but we gotta work with what we have. I do think I need to take it easy more. What I'm understanding though, we will have to make some complex queries and routes to satisfy the requirements of our project. Since it's a mini social media app that attempts to be a simpler version of Yelp, we have to process a lot of data. For example: getting a list of users by simply grab every user is not feasible. But doing a query of a substring that contains the characters in the username significantly reduces the search results and lightens how many records are sent back to the user from the backend. Large data going back and forth = BAD! Well, I may be thinking too deeply about this, but getting more acquainted with the idea of scalability only makes my understanding of the big picture better. I also remember that I was on such a roll today, that I forget to make regular commits throughout the day. Have to remind myself to make commits after every major file change.

11/17
I may have royally messed up by making the call to use MongoDB instead of PostgreSQL. I thought that using DBRef to act as pseudo foreign keys would be simple to manage. I also believed that directly adding some accounts with admin roles and other items such as categories, states, venues, and reviews at the initialization of the database (like what was done in the Books Library demo) would be simple. Alas, after extensive reading, I learned that such lookups to find the ObjectIds that reference items (example: a Venue holding a an ObjectId for as a category_id property where that category with the ObjectId is 'nightclub') is very taxing. As it stands, we most likely have to go back to the Postgres setup. As I'm the one who recommended to use Mongo, I will take the time to quickly get back the changes and incorporate all the models. It was very frustrating to learn this, and I feel like I'm very lost. Nonetheless, I will keep forging ahead. I plan on making a separate branch that will incorporate this massive change and have it looked at before we even think about connecting it to the main branch. There's so many unknowns and I can't lie: I'm terrified.

11/16
Today we started our Postgresql database and its tables. We were able to break down all the entities we have identified by far. It seems that a lot of tables are required to show complex relationships such as likes and follows. Based on my readings of implementations done by others, having separate tables for these actions that can be done to multiple items (reviews to be liked and users to be followed) makes the data compliant to RDBMS standards. We are wary about how this works with user accounts where there are two roles: admin and standard user. We will consult Riley in the morning. We have also identified two possible venue address validating APIs, YAddress (requires simple JSON) and the USPS Address Validator (requires complex XML). I believe YAddress simplifies most of the work in a easy to code fashion, but has up to 1000 free requests.

11/15
We're getting a better understanding of what tables our data will rely on. We are considering using the accounts feature as part of the main backend microservice instead of polling AccountVOs from a separate accounts microservice. The issue is understanding how we can assign users to their account profiles. We plan on going through more of the explorations, especially on authentication, and look into the Books demo project to see how accounts and roles were implemented.

11/14
Today we worked on finishing up the exploration on databases and FastAPI.
We have a solid understanding of what core features our application rely on.
We will continue to brush up on FastAPI in preparation for beginning our code
tomorrow. I really look forward to seeing how we break down our accounts microservice.
